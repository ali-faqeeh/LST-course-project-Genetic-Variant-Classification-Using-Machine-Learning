{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from ftfy import fix_encoding\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data and intial variable\n",
    "train_var  = pd.read_csv('training_variants')\n",
    "train_text = pd.read_csv('training_text', sep='\\|\\|', engine='python')\n",
    "test_var   = pd.read_csv('test_variants')\n",
    "test_text  = pd.read_csv('test_text', sep='\\|\\|', engine='python')\n",
    "\n",
    "# Drop the rows whose text is NA and correct index in df after dropping elements\n",
    "fixed_train_var        = train_var.drop([1109, 1277, 1407, 1639, 2755])\n",
    "fixed_train_var.index  = range(len(fixed_train_var))                \n",
    "fixed_train_text       = train_text.drop([1109, 1277, 1407, 1639, 2755])\n",
    "fixed_train_text.index = range(len(fixed_train_text))\n",
    "\n",
    "fixed_test_var        = test_var.drop(1623)\n",
    "fixed_test_var.index  = range(len(fixed_test_var))\n",
    "fixed_test_text       = test_text.drop(1623)\n",
    "fixed_test_text.index = range(len(fixed_test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "def clean(text):\n",
    "    \n",
    "    # Document-level processing:\n",
    "    stop = set(stopwords.words('english'))\n",
    "    sample = text   \n",
    "    \n",
    "    sample = sample.lower()                         # Lower case\n",
    "    \n",
    "    sample = sample.replace('fig. ', 'fig.')        # Return 'figure' to a format that can be easily processed in the sentence-level\n",
    "    sample = sample.replace('figure ', 'fig.')\n",
    "    \n",
    "    regex  = re.compile(\"\\w+\\S\\w+|\\w+|[!?,-.:]\")    # Regular expression for format the sentence structure as ' sign ' for better filtering\n",
    "    sample = re.findall(regex, sample)              # Find all sentences meet the requirement of the regular expression\n",
    "    \n",
    "    sample = filter(lambda val: val != ',', sample)             # Remove ','\n",
    "    sample = [w for w in sample if w not in stop]               # Remove stopwords \n",
    "    sample = [w for w in sample if 'supplementary' not in w]    # Remove 'supplementary' \n",
    "    sample = [w for w in sample if 'shown' not in w]            # Remove 'shown'\n",
    "    sample = \" \".join(sample)                                   # Ensemble the document after regex\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Sentence-level processing:\n",
    "    corpus     = sample.split(' . ') # Or sentences\n",
    "    new_corpus = []                  # Collection of senteces satisfying conditions\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        \n",
    "        sentence_trim = re.sub(\"\\([^()]*\\)\", \"\", sentence)            # Remove contents from parenthesis and square brackets in a sentence\n",
    "        words         = sentence_trim.split()                         # Words from a sentence\n",
    "        year          = [format(x, '02d') for x in range(1950,2022)]  # Getting the 'year' list ranging from 1950 to 2022 \n",
    "        \n",
    "        \n",
    "        # Filter out short sentences less than 5 words\n",
    "        if len(words) >= 5:\n",
    "            \n",
    "            # Clean the years\n",
    "            if words[0] in year:                                     \n",
    "                del words[0]    \n",
    "                \n",
    "            # Clean references(X-et-al)\n",
    "            if words[-1] == 'al':                                    \n",
    "                del words[-3:]\n",
    "                \n",
    "            # Clean fig. / tables\n",
    "            words = [w for w in words if 'fig.' not in w]           # Initial filter\n",
    "            fig_tab = ['fig','figs','figures','table','tables']     # Back filter\n",
    "            for idx in range(-4,0,1):\n",
    "                if len(words) >= -idx+1 and words[idx] in fig_tab:\n",
    "                    del words[idx:]\n",
    "                        \n",
    "            # Remove repeated 'download figureopen new tabdownload powerpoint' from the figures' notes\n",
    "            while len(words) != 0 and words[0] == 'download':\n",
    "                del words[0:5]\n",
    "            \n",
    "            # Setting threshold for AminoAcid case(collection of short words)\n",
    "            len_words =  [len(char) for char in words]      \n",
    "            small_w   = [len_words.count(1),len_words.count(2),len_words.count(3)]\n",
    "            threshold = [8,10,6]   # Can be tweaked     \n",
    "            amino     = any([count > limit for count, limit in zip(small_w,threshold)])  \n",
    "            \n",
    "            # Omit short final result which contains lengthy words/nucleotides seq or too short \n",
    "            sentence_trim = \" \".join(words)\n",
    "            if len(sentence_trim) > 30:                              \n",
    "                if len(max(words, key=len)) < 20:\n",
    "                    if amino == False:\n",
    "                        new_corpus.append(sentence_trim) \n",
    "                    \n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n# Run this code for getting the new data schema\\n\\nrow = fixed_train_text.shape[0]\\nclean_text = []\\nfor i in range(0,row):\\n    input_str = \" . \".join(clean(fixed_trin_text.iloc[i][0]))\\n    clean_text.append(input_str)\\n\\nresult = {\\'clean_text\\': corpus}\\ndf = pd.DataFrame(result) \\ndf\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "# Run this code for getting the new data schema\n",
    "\n",
    "row        = fixed_train_text.shape[0]\n",
    "clean_text = []\n",
    "\n",
    "for i in range(0,row):\n",
    "    processed_input = clean(fixed_trin_text.iloc[i][0])\n",
    "    output_str      = \" . \".join(processed_input)\n",
    "    clean_text.append(output_str)\n",
    "\n",
    "result = {'clean_text': clean_text}\n",
    "df     = pd.DataFrame(result) \n",
    "df\n",
    "\n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple expansion for variation text and fix encoding\n",
    "def expansion(df):\n",
    "    clone    = df\n",
    "    var_text = []\n",
    "    row      = len(clone)\n",
    "    # Expansion of variation column\n",
    "    for i in range(row):\n",
    "        sentence = clone.iloc[i][3].split(' . ')\n",
    "        target   = clone.iloc[i][1]\n",
    "        output   = []\n",
    "        for idx in range(len(sentence)):\n",
    "             if (target.lower() in sentence[idx]):\n",
    "                output.append(sentence[idx])\n",
    "        var_text.append(\" . \".join(output))\n",
    "    clone['var_text'] = var_text\n",
    "    # Fix encoding\n",
    "    for j in range(row):\n",
    "        clean_text = clone.iloc[i][3] \n",
    "        var_text   = clone.iloc[i][4] \n",
    "        clone.at[i,'clean_text'] = fix_encoding(clean_text)\n",
    "        clone.at[i,'var_text']   = fix_encoding(var_text)\n",
    "\n",
    "    return clone   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n# Run this code for getting the new data schema\\n\\ncombine_train    = pd.concat([fixed_train_var.drop([\\'ID\\'],axis=1),df],axis=1)\\nexpansion(combine_train)\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "# Run this code for getting the new data schema\n",
    "\n",
    "df            = df\n",
    "combine_train = pd.concat([fixed_train_var.drop(['ID'],axis=1),df],axis=1)\n",
    "new_schema    = expansion(combine_train)\n",
    "new_schema.to_csv('New_train_data.csv') \n",
    "\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a table report all duplicated cases\n",
    "def report_dup(data):\n",
    "    # Setting up variables:\n",
    "    sorted_data = data.sort_values(by = ['ID,Text'])\n",
    "    rows        = sorted_data.shape[0]\n",
    "    final_text  = []\n",
    "    duplicate   = []\n",
    "    i = 0\n",
    "    # First loop is to go through each text\n",
    "    while i != rows-1:\n",
    "        if clean(sorted_data.iloc[i][0])[0] == clean(sorted_data.iloc[i+1][0])[0]: # whether the first sentences of two adjacent documents are the same \n",
    "            text  = sorted_data.iloc[i][0]\n",
    "            index = [sorted_data.index[i]]\n",
    "            k = 1 # index k of duplicated elements for a text\n",
    "            if i == rows-2:\n",
    "                index.append(sorted_data.index[i+1])\n",
    "            else:\n",
    "                # Second loop is to count how many texts are consecutively the same as the target text\n",
    "                while clean(sorted_data.iloc[i+k][0])[0] == clean(sorted_data.iloc[i][0])[0] and i+k < rows-1: \n",
    "                    index.append(sorted_data.index[i+k])\n",
    "                    k += 1\n",
    "            final_text.append(text)\n",
    "            duplicate.append(index)\n",
    "            i += k\n",
    "        else:\n",
    "            lone_text = sorted_data.iloc[i][0]\n",
    "            lone_index = [sorted_data.index[i]]\n",
    "            final_text.append(lone_text)\n",
    "            duplicate.append(lone_index)\n",
    "            i += 1\n",
    "        \n",
    "    final_table = pd.DataFrame(list(zip(final_text, duplicate)), columns = ['Text', 'List_of_index'])\n",
    "    count_table = final_table.assign(Duplicates = final_table.List_of_index.apply(lambda x: len(x)))    # Count number of duplicates\n",
    "    extreme = count_table.sort_values(by = ['Duplicates'], ascending =0)                                # Sort the resulting table to observe extreme cases\n",
    "    return extreme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to avoid non-unique variation names when checking the distribution\n",
    "stop_var = ['truncating mutations','deletion','promoter mutations','amplification','promoter hypermethylation','overexpression','epigenetic silencing','dna binding domain deletions','dna binding domain insertions','dna binding domain missense mutations','copy number loss','hypermethylation','wildtype','fusions']\n",
    "\n",
    "# Extract info from variation data\n",
    "var_gene  = fixed_train_var.iloc[:,1]\n",
    "var_df    = fixed_train_var.iloc[:,2]\n",
    "var_class = fixed_train_var.iloc[:,3]\n",
    "\n",
    "\n",
    "combine = list(zip(var_df,var_class,var_gene))\n",
    "combine_2 = map(lambda x: (x[0].lower(),x[1],x[2]),combine)   # Lowercase the variation names\n",
    "combine_3 = filter(lambda x: x[0] not in stop_var,combine_2)  # Filter out variations that collides with stop_var\n",
    "var_dict = list(OrderedDict.fromkeys(combine_3))              # Dictionary of variations -> class/gene\n",
    "\n",
    "# Dictionary for class -> color when plotting\n",
    "class_color = dict(zip([1,2,3,4,5,6,7,8,9], ['r','g','b','y','m','c','k','grey','cyan']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_report(data,idx):\n",
    "    \n",
    "    print('\\033[1m' +'Case '+str(idx) + '\\n'+ '\\033[0m')\n",
    "    \n",
    "    # Define intial variable\n",
    "    text       = data.iloc[idx][0]\n",
    "    document   = clean(text)\n",
    "    var_gene   = fixed_train_var.iloc[:,1]\n",
    "    major_gene = Counter(map(lambda x: var_gene[x], data.iloc[idx][1])).most_common(1)[0][0]\n",
    "    \n",
    "    ranges     = list(range(len(document)))\n",
    "    all_res    = []\n",
    "    report     = []\n",
    "    \n",
    "    # Brief description on the target document:\n",
    "    print('\\033[1m' +'Brief description on the document:' + '\\033[0m')\n",
    "    print(data.iloc[idx])\n",
    "    print('')\n",
    "    print('')\n",
    "    \n",
    "    print('\\033[1m' +'Report on detection of multiple entities in a sentence and deletion of same variant with different class:' + '\\033[0m')\n",
    "    # Looping thru each sentence\n",
    "    for i in range(len(document)):\n",
    "        sentence = document[i]\n",
    "        candidate = []\n",
    "        exist = []\n",
    "        \n",
    "        # Finding variaitons appearing in a sentence\n",
    "        for j in range(len(var_dict)):\n",
    "            if var_dict[j][0] in sentence:\n",
    "                candidate.append(j)\n",
    "\n",
    "        name   = list(map(lambda x: var_dict[x][0], candidate))\n",
    "        classV = list(map(lambda x: var_dict[x][1], candidate))\n",
    "        typeG  = list(map(lambda x: var_dict[x][2], candidate))\n",
    "        \n",
    "        # Report on multiple names appearing in a sentence\n",
    "        if len(candidate) > 1:\n",
    "            classV_str = list(map(str, classV))\n",
    "            print('Multiple names are mentioned at sentence ' + str(i) + ' like ' + ', '.join(name) + ' from class ' + ', '.join(classV_str) + ' respectively.\\n')\n",
    "        \n",
    "        # If overlaps happens to have same name mentioned then probably there exist a variaiton whose genes are shared, check with the majority gene and excludes the other possibilities\n",
    "        overlap = [item for item, count in collections.Counter(name).items() if count > 1]   \n",
    "        if len(overlap) > 0:\n",
    "            delete = []\n",
    "            for k in range(len(name)):\n",
    "                if name[k] in overlap:\n",
    "                    if typeG[k] != major_gene:\n",
    "                        delete.append(candidate[k])\n",
    "                        print('Delete ' + name[k] +' from ' + typeG[k] + ' from class ' + str(classV[k]) + ' at sentence ' + str(i))\n",
    "                        print('')\n",
    "            candidate = [x for x in candidate if x not in delete]\n",
    "    \n",
    "        # Finalize the final report of variations for a sentence\n",
    "        for val in candidate:\n",
    "            report.append((var_dict[val][0],var_dict[val][1],var_dict[val][2]))\n",
    "            exist.append(var_dict[val][1])\n",
    "        all_res.append(exist)\n",
    "    \n",
    "    \n",
    "    # Plotting the distribution using different colors for each class\n",
    "    for xe, ye in zip(ranges,all_res):\n",
    "        if len(ye) > 0:\n",
    "            for i in range(len(ye)):\n",
    "                plt.scatter(xe,ye[i], c=class_color.get(ye[i]))\n",
    "    print('')\n",
    "    print('')\n",
    "    print('\\033[1m' + 'Plot for extreme Case ' + str(idx)+'\\033[0m')\n",
    "    plt.xlabel(\"x_th sentence in the document\")\n",
    "    plt.ylabel(\"Classes\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Report on the mentioned variations in the text\n",
    "    print('\\033[1m' + 'Report on variants in the document' + str(idx)+'\\033[0m')\n",
    "    report = list(OrderedDict.fromkeys(report))\n",
    "    classes =  list(OrderedDict.fromkeys([x[1] for x in report]))\n",
    "    for idx in classes:\n",
    "        variation = filter(lambda x: x[1] == idx,report)\n",
    "        var_gene = [x[0] +' ('+ x[2] +')' for x in variation]\n",
    "        print('Class ' + str(idx) + ' : ' +  ', '.join([text for text in var_gene]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\\n\\n# Run this code for reporting the distribution of variations for case ?th\\n\\nextreme = report_dup(fixed_train_text)\\nindex = ? #Index for the most ?th extreme case (max 1323 / 1322 for input)\\ndis_report(extreme,index)\\n\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\"\n",
    "\n",
    "# Run this code for reporting the distribution of variations for case ?th\n",
    "\n",
    "extreme = report_dup(fixed_train_text)\n",
    "index = ? #Index for the most ?th extreme case (max 1323 / 1322 for input)\n",
    "dis_report(extreme,index)\n",
    "\n",
    "\"\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
